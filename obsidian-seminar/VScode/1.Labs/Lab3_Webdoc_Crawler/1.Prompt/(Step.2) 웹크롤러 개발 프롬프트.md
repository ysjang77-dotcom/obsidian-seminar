# **역할 (Persona)**

당신은 Python을 활용한 웹 크롤링 및 데이터 자동화 처리 분야의 전문 소프트웨어 엔지니어입니다. 특히 `requests`, `BeautifulSoup`, `html2text` 라이브러리에 매우 능숙하며, 단일 책임 원칙(SRP)과 같은 소프트웨어 공학 원칙을 준수하여 모듈화되고 재사용성 높은 코드를 작성하는 데 특화되어 있습니다. 또한, 사용자가 최종 결과물을 쉽게 이해하고 활용할 수 있도록 상세한 주석과 문서화 작업을 중요하게 생각합니다.

# **작업 목표 (Overall Goal)**

사용자가 입력한 특정 웹페이지 URL을 기준으로, 해당 페이지에 링크된 모든 하위 HTML 문서를 재귀적으로 크롤링하고, 각 문서에서 핵심 콘텐츠만 추출하여 하나의 통합된 마크다운(`.md`) 파일로 생성하는 Python 스크립트를 작성합니다. 모든 과정은 단계별로 로그에 기록되어야 하며, 최종 사용자가 쉽게 실행하고 유지보수할 수 있도록 설계되어야 합니다.

# **단계별 실행 계획 및 상세 요구사항 (Plan-and-Solve & Task)**

이 작업을 성공적으로 완수하기 위해, 다음의 논리적 단계를 순서대로 생각하며 코드를 작성해 주세요.

### **1단계: 프로젝트 구조 및 환경 설정**

1. **파일 구조 정의**: 최종 결과물은 다음과 같은 구조로 생성되어야 합니다. 사용자가 스크립트를 실행하면 이 구조가 자동으로 생성되도록 코드를 작성해 주세요.
    
    ```
    /output/
    ├── crawled_html/      # 크롤링된 개별 HTML 파일 저장 폴더
    │   ├── page1.html
    │   ├── page2.html
    │   └── ...
    ├── integrated_documentation.md  # 최종 통합 마크다운 문서
    └── crawl_log.log        # 실행 과정 기록 로그 파일
    ```
    
2. **라이브러리 의존성 관리**: 코드 실행에 필요한 모든 외부 라이브러리 목록을 `requirements.txt` 파일로 명시해 주세요. (예: `requests`, `beautifulsoup4`, `html2text`)
    

### **2단계: 입력 처리 및 설정**

1. **URL 입력 처리**: 사용자가 크롤링을 시작할 최상위 페이지 URL을 코드 외부에서 쉽게 지정할 수 있도록 `argparse` 라이브러리를 사용하여 커맨드 라인 인자로 받도록 구현해 주세요.
    
2. **설정 관리**: 출력 폴더명, 로그 파일명, 크롤링 대상 링크를 필터링하기 위한 조건(예: 특정 경로 패턴) 등은 스크립트 상단의 설정(Configuration) 영역에 상수로 분리하여 관리의 용이성을 높여주세요.
    

### **3단계: 핵심 크롤링 및 파싱 로직 구현 (모듈화)**

각 기능은 단일 책임 원칙(SRP)을 준수하는 별도의 함수로 모듈화하여 가독성과 재사용성을 극대화해 주세요.

1. **메인 페이지 링크 추출 함수 (`extract_main_links`)**:
    
    - 입력: 최상위 페이지 URL
        
    - 기능:
        
        - 주어진 URL의 HTML을 가져옵니다.
            
        - `BeautifulSoup`을 사용하여 HTML을 파싱합니다.
            
        - 페이지 내의 모든 `<a>` 태그를 찾되, 분석 대상이 되는 하위 문서 링크(예: 상대 경로이며 `.html`로 끝나는 링크)만 선별적으로 추출하여 리스트로 반환합니다. 외부로 나가는 링크나 페이지 내의 앵커(#) 링크는 제외해야 합니다.
            
        - **[논리 보완]**: 추출된 상대 경로는 나중에 완전한 URL로 조합할 수 있도록 기반 URL(base URL) 정보도 함께 관리해야 합니다.
            
2. **하위 페이지 처리 및 저장 함수 (`process_and_save_subpage`)**:
    
    - 입력: 하위 페이지 URL
        
    - 기능:
        
        - 해당 URL의 HTML 콘텐츠를 요청하고 가져옵니다.
            
        - **[불필요한 부분 제거]**: HTML 문서에서 실제 내용이 담긴 핵심 컨테이너(예: `<article>`, `<main>`, `<div role="main">` 등)를 식별하여 해당 부분만 추출합니다. 이는 네비게이션 바, 사이드바, 푸터 등의 불필요한 요소를 효과적으로 제거하기 위함입니다. (어떤 태그를 기준으로 할지 분석하고, 이를 설정 영역에서 변경 가능하도록 설계하면 더욱 좋습니다.)
            
        - 추출된 핵심 HTML 콘텐츠를 `output/crawled_html/` 폴더에 원본 파일명과 유사한 이름으로 저장합니다.
            
        - `html2text` 라이브러리를 사용하여 정제된 HTML을 마크다운 형식의 텍스트로 변환하여 반환합니다.
            

### **4단계: 통합 및 출력**

1. **메인 실행 로직 (`main`)**:
    
    - 사용자로부터 입력받은 URL로 `extract_main_links` 함수를 호출하여 크롤링할 링크 목록을 가져옵니다.
        
    - 추출된 각 링크에 대해 반복문을 실행합니다.
        
    - 반복문 내에서 `process_and_save_subpage` 함수를 호출하여 각 하위 페이지의 내용을 마크다운으로 변환하고, 그 결과를 하나의 변수에 순차적으로 누적합니다.
        
    - 모든 링크 처리가 완료되면, 누적된 마크다운 텍스트를 `output/integrated_documentation.md` 파일에 저장합니다.
        

### **5단계: 안정성 및 사용자 편의성 강화 (제약 조건 및 비기능적 요구사항)**

1. **로깅 (Logging)**:
    
    - Python의 내장 `logging` 모듈을 사용해 주세요.
        
    - 파일 핸들러와 스트림 핸들러를 모두 설정하여, 로그가 `output/crawl_log.log` 파일과 콘솔에 동시에 출력되도록 합니다.
        
    - 기록 내용: 현재 어떤 URL을 크롤링 중인지, 어떤 파일을 저장했는지, 에러가 발생했다면 어떤 에러인지 등 실행 흐름을 추적할 수 있는 상세 정보를 `INFO` 레벨 이상으로 기록해 주세요.
        
2. **예외 처리 (Error Handling)**:
    
    - 네트워크 요청 시 발생할 수 있는 `ConnectionError` 나 HTTP `404 Not Found` 같은 에러를 `try-except` 구문으로 처리하여, 일부 페이지에서 문제가 발생하더라도 전체 스크립트가 중단되지 않도록 만들어 주세요. 에러 발생 시, 해당 URL과 에러 내용을 로그에 기록하고 다음 작업으로 넘어가야 합니다.
        
3. **코드 품질**:
    
    - 모든 함수에 대해 기능, 인자, 반환 값을 설명하는 명확한 Docstring(PEP 257 스타일)을 작성해 주세요.
        
    - 복잡한 로직에는 이해를 돕기 위한 인라인 주석을 추가해 주세요.
        

# **결과물 형식 (Format)**

최종 결과물로 아래의 파일들을 각각의 코드 블록으로 나누어 생성해 주세요.

1. **`main.py`**: 위 모든 요구사항이 반영된 메인 Python 스크립트 파일
    
2. **`requirements.txt`**: 스크립트 실행에 필요한 라이브러리 목록
    
3. **`README.md`**: 스크립트에 대한 간단한 설명, 설치 방법(`pip install -r requirements.txt`), 그리고 실행 방법(예: `python main.py <URL>`)을 안내하는 마크다운 문서
    
