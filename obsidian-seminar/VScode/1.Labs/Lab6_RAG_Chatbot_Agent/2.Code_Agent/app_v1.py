# ==============================================================================
# 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (Environment Setup & Import Libraries)
# ==============================================================================
import os
import streamlit as st
from dotenv import load_dotenv
import asyncio

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS

# asyncio ì´ë²¤íŠ¸ ë£¨í”„ ì„¤ì • (Streamlit ë¹„ë™ê¸° ë¬¸ì œ í•´ê²°)
# í˜„ì¬ ìŠ¤ë ˆë“œì— ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•˜ì—¬ ì„¤ì •í•©ë‹ˆë‹¤.
# ì´ëŠ” langchainì˜ ë¹„ë™ê¸° ê¸°ëŠ¥ì´ Streamlit í™˜ê²½ì—ì„œ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.
try:
    asyncio.get_running_loop()
except RuntimeError:  # 'RuntimeError: There is no current event loop...'
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
# ì´ í•¨ìˆ˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ë¶€ë¶„ì—ì„œ í•œ ë²ˆë§Œ í˜¸ì¶œí•˜ë©´ ë©ë‹ˆë‹¤.
load_dotenv()

# FAISS ì¸ë±ìŠ¤ ê²½ë¡œì™€ ì„ë² ë”© ëª¨ë¸ ì´ë¦„ì„ ìƒìˆ˜ë¡œ ì •ì˜í•©ë‹ˆë‹¤.
FAISS_INDEX_PATH = "faiss_index"
EMBEDDING_MODEL = "models/gemini-embedding-001"
LLM_MODEL = "gemini-2.5-flash" # Use a faster model for better interactivity

# ==============================================================================
# 2. RAG ì²´ì¸ ì´ˆê¸°í™” í•¨ìˆ˜ (Initialize RAG Chain Function)
# ==============================================================================

@st.cache_resource(show_spinner="ì „ë¬¸ê°€ ì‹œìŠ¤í…œì„ ì¤€ë¹„í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
def get_rag_chain():
    """
    Streamlitì˜ ìºì‹œ ë¦¬ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ RAG ì²´ì¸ì„ ì´ˆê¸°í™”í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ëŠ” ì•±ì´ ì²˜ìŒ ì‹¤í–‰ë  ë•Œ í•œ ë²ˆë§Œ í˜¸ì¶œë˜ì–´ ëª¨ë¸ê³¼ VectorDB ë¡œë”©ì˜
    ë¹„íš¨ìœ¨ì„ ë°©ì§€í•©ë‹ˆë‹¤.
    """
    try:
        # --- API í‚¤ ë° í•„ìˆ˜ ê²½ë¡œ í™•ì¸ ---
        if not os.getenv("GOOGLE_API_KEY"):
            raise ValueError("'.env' íŒŒì¼ì— GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        if not os.path.exists(FAISS_INDEX_PATH):
            raise FileNotFoundError(f"FAISS ì¸ë±ìŠ¤ í´ë” '{FAISS_INDEX_PATH}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € VectorDBë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.")

        # --- 1. ì„ë² ë”© ëª¨ë¸ ë° LLM ì´ˆê¸°í™” ---
        embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL)
        llm = ChatGoogleGenerativeAI(model=LLM_MODEL, temperature=0, convert_system_message_to_human=True)

        # --- 2. ë¡œì»¬ FAISS VectorDB ë¡œë“œ ---
        # allow_dangerous_deserialization=TrueëŠ” ë¡œì»¬ í™˜ê²½ì˜ pkl íŒŒì¼ì„ ì‹ ë¢°í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        vectorstore = FAISS.load_local(
            FAISS_INDEX_PATH, 
            embeddings, 
            allow_dangerous_deserialization=True
        )

        # --- 3. Retriever ìƒì„± ---
        # VectorDBë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” Retrieverë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

        # --- 4. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜ (í˜ë¥´ì†Œë‚˜ ì ìš©) ---
        # ì‹ ë¢°ì„± ê³µí•™ ì „ë¬¸ê°€ì˜ í˜ë¥´ì†Œë‚˜ë¥¼ ì •ì˜í•˜ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤.
        system_prompt = """
        ë‹¹ì‹ ì€ ìˆ˜ì‹­ ë…„ ê²½ë ¥ì˜ ì‹ ë¢°ì„± ê³µí•™(Reliability Engineering) ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ê¸°ìˆ  ë¬¸ì„œ(Context)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë§¤ìš° ë…¼ë¦¬ì ì´ê³ , ì²´ê³„ì ì´ë©°, ì‹¬ì¸µì ì¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

        **ë‹µë³€ ìƒì„± ê·œì¹™:**
        1.  **ì „ë¬¸ê°€ì  ë¶„ì„:** ë‹¨ìˆœíˆ ì •ë³´ë¥¼ ìš”ì•½í•˜ì§€ ë§ê³ , ê° ì •ë³´ì˜ ì¸ê³¼ê´€ê³„, ì¤‘ìš”ë„, ì ì¬ì  ë¦¬ìŠ¤í¬ ë“±ì„ ë¶„ì„í•˜ì—¬ ì „ë¬¸ê°€ì  ê²¬í•´ë¥¼ í¬í•¨í•˜ì„¸ìš”.
        2.  **ê°ê´€ì  ê·¼ê±° ì œì‹œ:** ë‹µë³€ì˜ ëª¨ë“  ë‚´ìš©ì€ ë°˜ë“œì‹œ ì£¼ì–´ì§„ [Context]ì— ê·¼ê±°í•´ì•¼ í•˜ë©°, ì–´ë–¤ ë¬¸ì„œë¥¼ ì°¸ê³ í–ˆëŠ”ì§€ ì¶œì²˜(source)ë¥¼ ëª…í™•í•˜ê²Œ ëª…ì‹œí•´ì•¼ í•©ë‹ˆë‹¤.
        3.  **ì¶”ê°€ ì¶”ë¡  ë° ì œì•ˆ:** ì‚¬ìš©ìê°€ ì¶”ê°€ ë¶„ì„ì„ ìš”ì²­í•˜ë©´, ê³¼ê±° ëŒ€í™” ë‚´ìš©ì„ ëª¨ë‘ ê³ ë ¤í•˜ì—¬ ì¢…í•©ì ì¸ ê²°ë¡ ì„ ë„ì¶œí•˜ê³ , í•„ìš”í•˜ë‹¤ë©´ ì¶”ê°€ì ì¸ ë¶„ì„ ë°©ë²•ì´ë‚˜ ëŒ€ì±…ì„ ì œì•ˆí•˜ì„¸ìš”.
        4.  **ëª¨ë¥´ëŠ” ì •ë³´:** [Context]ì— ì—†ëŠ” ë‚´ìš©ì— ëŒ€í•œ ì§ˆë¬¸ì—ëŠ” "ì£¼ì–´ì§„ ì •ë³´ë§Œìœ¼ë¡œëŠ” ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."ë¼ê³  ëª…í™•íˆ ë°íˆì„¸ìš”.
        """

        # --- 5. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„± (ëŒ€í™” ê¸°ë¡ í¬í•¨) ---
        # ëŒ€í™” ê¸°ë¡(chat_history)ê³¼ ì‚¬ìš©ì ì§ˆë¬¸(human_input)ì„ ì²˜ë¦¬í•˜ëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "ì§ˆë¬¸: {human_input}\n\nì°¸ê³ í•  ë¬¸ì„œ:\n{context}"),
        ])

        # --- 6. ê²€ìƒ‰ëœ ë¬¸ì„œ í¬ë§·íŒ… í•¨ìˆ˜ ---
        def format_docs(docs):
            # ê²€ìƒ‰ëœ ë¬¸ì„œ ê°ì²´ë¥¼ í”„ë¡¬í”„íŠ¸ì— ë§ê²Œ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.
            return "\n\n".join(f"ì¶œì²˜: {doc.metadata.get('source', 'N/A')}\n---\n{doc.page_content}" for doc in docs)

        # --- 7. RAG ì²´ì¸ êµ¬ì„± (LCEL) ---
        # LangChain Expression Language (LCEL)ë¥¼ ì‚¬ìš©í•˜ì—¬ RAG íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
        rag_chain = (
            RunnablePassthrough.assign(
                context=lambda x: format_docs(retriever.invoke(x["human_input"])),
            )
            | prompt
            | llm
            | StrOutputParser()
        )
        
        return rag_chain

    except (ValueError, FileNotFoundError) as e:
        # ì„¤ì • ì˜¤ë¥˜ ë°œìƒ ì‹œ ì‚¬ìš©ìì—ê²Œ ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•˜ê³  ì•± ì‹¤í–‰ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.
        st.error(f"ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None
    except Exception as e:
        # ê¸°íƒ€ ì˜ˆì™¸ ì²˜ë¦¬
        st.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None

# ==============================================================================
# 3. Streamlit UI ê¸°ë³¸ êµ¬ì¡° ì„¤ì • (Streamlit UI Base Setup)
# ==============================================================================

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(page_title="ì‹ ë¢°ì„± ë¶„ì„ ì „ë¬¸ê°€ ì±—ë´‡", page_icon="ğŸ¤–")
st.title("ğŸ¤– ì‹ ë¢°ì„± ë¶„ì„ ì „ë¬¸ê°€ ì±—ë´‡")
st.markdown("""
ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì‹ ë¢°ì„± ê¸°ìˆ ìë£Œ DBë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¬ì¸µ ë¶„ì„ì„ ì œê³µí•˜ëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤.
íŒŒë…¸ë¼ë§ˆ ì„ ë£¨í”„, ë³¼íœ ë“± ë‹¤ì–‘í•œ ì œí’ˆì˜ ê³ ì¥ ë¶„ì„ ë³´ê³ ì„œì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”.
""")

# --- ëŒ€í™” ê¸°ë¡(Session State) ì´ˆê¸°í™” ---
if "messages" not in st.session_state:
    st.session_state.messages = [
        AIMessage(content="ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì‹ ë¢°ì„± ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    ]

# --- RAG ì²´ì¸ ê°€ì ¸ì˜¤ê¸° ---
# ìºì‹±ëœ RAG ì²´ì¸ì„ ë¡œë“œí•©ë‹ˆë‹¤. ì˜¤ë¥˜ ë°œìƒ ì‹œ rag_chainì€ Noneì´ ë©ë‹ˆë‹¤.
rag_chain = get_rag_chain()

# ==============================================================================
# 4. ëŒ€í™” ë‚´ìš© í‘œì‹œ ë° ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ (Display & Process Conversation)
# ==============================================================================

# --- ì´ì „ ëŒ€í™” ë‚´ìš© í‘œì‹œ ---
for message in st.session_state.messages:
    if isinstance(message, AIMessage):
        with st.chat_message("AI"):
            st.markdown(message.content)
    elif isinstance(message, HumanMessage):
        with st.chat_message("Human"):
            st.markdown(message.content)

# --- ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ---
# ì‚¬ìš©ìê°€ ì±„íŒ… ì…ë ¥ì°½ì— ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ë©´ ì•„ë˜ ì½”ë“œê°€ ì‹¤í–‰ë©ë‹ˆë‹¤.
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."):
    # RAG ì²´ì¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
    if rag_chain:
        # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€í•˜ê³  í™”ë©´ì— í‘œì‹œ
        st.session_state.messages.append(HumanMessage(content=prompt))
        with st.chat_message("Human"):
            st.markdown(prompt)

        # AI ë‹µë³€ ìƒì„± ë° í‘œì‹œ (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹)
        with st.chat_message("AI"):
            # st.write_streamì€ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ì„ ì²˜ë¦¬í•˜ê³ , ì „ì²´ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
            # ì´ë¥¼ í†µí•´ ì‚¬ìš©ìëŠ” ë‹µë³€ì´ ìƒì„±ë˜ëŠ” ê³¼ì •ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            full_response = st.write_stream(rag_chain.stream({
                "human_input": prompt,
                "chat_history": st.session_state.messages
            }))
        
        # AI ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€ (ìŠ¤íŠ¸ë¦¬ë°ì´ ì™„ë£Œëœ ì „ì²´ ë©”ì‹œì§€)
        st.session_state.messages.append(AIMessage(content=full_response))
    else:
        # RAG ì²´ì¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©ìì—ê²Œ ì•ˆë‚´
        st.error("ì±—ë´‡ ì‹œìŠ¤í…œì´ ì •ìƒì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
