# tools.py
import io
import os
import json
import re
import contextlib
from typing import Dict, Any, List, Union

import streamlit as st
from langchain.tools import tool
import pandas as pd

from RA_code_v6 import (
    preprocess_and_summarize_data, find_best_distribution,
    analyze_single_distribution, check_parameter_homogeneity, setup_environment
)

setup_environment()

# --- Argument Parsing Helper Functions ---

def parse_string_input(input_data: Union[str, dict]) -> str:
    """Robustly parses string or dict input from LLM to get the actual string value."""
    if isinstance(input_data, dict):
        # If input is a dict, extract the first value.
        return str(next(iter(input_data.values()))) if input_data else ""
    
    if isinstance(input_data, str):
        try:
            # Try to load if it's a JSON string representing a dict
            parsed = json.loads(input_data.replace("'", '"'))
            if isinstance(parsed, dict):
                return str(next(iter(parsed.values())))
        except (json.JSONDecodeError, TypeError):
            # If it's not a dict-like JSON, return the string itself.
            return input_data.strip()
    
    return str(input_data)

def parse_blives_input(input_data: Union[List, str, None]) -> List[float]:
    """Robustly parses various B-life inputs (e.g., "B10", 10, "10") into a list of floats."""
    if input_data is None: return []
    
    if isinstance(input_data, list):
        processed_list = []
        for item in input_data:
            if isinstance(item, str):
                numbers = re.findall(r'\d+\.?\d*', item)
                if numbers:
                    processed_list.append(float(numbers[0]))
            elif isinstance(item, (int, float)):
                processed_list.append(float(item))
        return processed_list

    if isinstance(input_data, str):
        numbers = re.findall(r'\d+\.?\d*', input_data)
        return [float(num) for num in numbers]
        
    return []

def parse_list_input(input_data: Union[List, str, None]) -> List:
    if input_data is None: return []
    if isinstance(input_data, list): return input_data
    if isinstance(input_data, str):
        try:
            if ":" in input_data:
                parsed_json = json.loads(input_data.replace("'", '"'))
                return next(iter(parsed_json.values()))
            return json.loads(input_data.replace("'", '"'))
        except (json.JSONDecodeError, TypeError):
            return [item.strip() for item in input_data.split(',')]
    return []

# --- Tools ---

@tool
def data_summarizer_tool() -> Dict[str, Any]:
    """ì‚¬ìš©ìê°€ ë°ì´í„° ìš”ì•½, ì „ì²˜ë¦¬, ë˜ëŠ” ê·¸ë£¹ë³„ ìƒ˜í”Œ ìˆ˜ í™•ì¸ì„ ìš”ì²­í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤."""
    st.write("ğŸ§° **Tool Executing:** `data_summarizer_tool`")
    required_keys = ['uploaded_file_path', 'column_map', 'status_indicators']
    if not all(key in st.session_state and st.session_state[key] for key in required_keys):
        return {"error": "íŒŒì¼ ì—…ë¡œë“œ ë° ì»¬ëŸ¼ ì„¤ì •ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
    try:
        output_buffer = io.StringIO()
        with contextlib.redirect_stdout(output_buffer):
            grouped_data_from_func, summary_df = preprocess_and_summarize_data(
                file_path=st.session_state['uploaded_file_path'],
                column_map=st.session_state['column_map'],
                status_indicators=st.session_state['status_indicators']
            )
        
        # --- ë°ì´í„° í˜•ì‹ ë³´ì • ë¡œì§ --- #
        # preprocess_and_summarize_dataê°€ ë°˜í™˜í•˜ëŠ” ê°’ì˜ í˜•ì‹ì´ pandas Seriesì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
        # ëª¨ë“  ë„êµ¬ê°€ ì•ˆì •ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìˆœìˆ˜ Python listë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        cleaned_grouped_data = {}
        for group, data in grouped_data_from_func.items():
            cleaned_grouped_data[group] = {
                'failures': list(data['failures']),
                'right_censored': list(data['right_censored'])
            }

        st.session_state['grouped_data'] = cleaned_grouped_data
        # --- ë³´ì • ë¡œì§ ë --- #

        return {"dataframe": summary_df.to_dict('records'), "summary_text": output_buffer.getvalue()}
    except Exception as e:
        return {"error": f"ë°ì´í„° ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}

@tool
def best_distribution_finder_tool(distributions_to_fit: Union[List[str], str]) -> Dict[str, Any]:
    """ì‚¬ìš©ìê°€ ì£¼ì–´ì§„ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ ìµœì  ìˆ˜ëª…ë¶„í¬, ê°€ì¥ ì˜ ë§ëŠ” ë¶„í¬, ë˜ëŠ” ì—¬ëŸ¬ ë¶„í¬ ë¹„êµë¥¼ ìš”ì²­í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤."""
    st.write(f"ğŸ§° **Tool Executing:** `best_distribution_finder_tool`")
    try:
        distributions = parse_list_input(distributions_to_fit)
        if not distributions:
            distributions = ["Weibull_2P", "Lognormal_2P", "Normal_2P", "Exponential_1P"]
    except Exception as e:
        return {"error": f"ë¶„í¬ ë¦¬ìŠ¤íŠ¸ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}"}
    if 'grouped_data' not in st.session_state:
        return {"error": "ë°ì´í„°ê°€ ë¨¼ì € ìš”ì•½ë˜ì–´ì•¼ í•©ë‹ˆë‹¤."}
    try:
        output_buffer = io.StringIO()
        with contextlib.redirect_stdout(output_buffer):
            analysis_results, bic_df = find_best_distribution(st.session_state['grouped_data'], distributions)
        st.session_state['detailed_analysis_results'] = analysis_results
        plot_paths = [os.path.join("results", f) for f in ["probability_plot_all_groups.png", "BIC_comparison.png"]]
        return {"dataframe": bic_df.to_dict('records'), "results_text": output_buffer.getvalue(), "plot_paths": plot_paths}
    except Exception as e:
        return {"error": f"ìµœì  ë¶„í¬ íƒìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}

@tool
def detailed_distribution_analyzer_tool(
    group_name: Union[str, dict] = None, 
    distribution_name: str = None, 
    b_lives: Union[List, str] = None, 
    failure_prob_times: Union[List, str] = None
) -> Dict[str, Any]:
    """ì‚¬ìš©ìê°€ íŠ¹ì • ê·¸ë£¹ê³¼ ë¶„í¬ì— ëŒ€í•´ ìƒì„¸ ë¶„ì„, B-ìˆ˜ëª…(B10 ë“±), ë˜ëŠ” ê³ ì¥ í™•ë¥  ê³„ì‚°ì„ ìš”ì²­í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤. 'group_name'ê³¼ 'distribution_name'ì€ í•„ìˆ˜ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ê·¸ë£¹ ì´ë¦„ì„ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´, ë‹¤ì‹œ ì§ˆë¬¸í•´ì•¼ í•©ë‹ˆë‹¤."""
    st.write(f"ğŸ§° **Tool Executing:** `detailed_distribution_analyzer_tool`")
    
    args = {}
    if isinstance(group_name, dict): args = group_name
    elif isinstance(group_name, str) and group_name.strip().startswith('{'):
        try: args = json.loads(group_name.replace("'", '"'))
        except json.JSONDecodeError: pass

    final_group_name = args.get('group_name', group_name if isinstance(group_name, str) and not group_name.strip().startswith('{') else None)
    final_dist_name = args.get('distribution_name', distribution_name)
    
    if not final_group_name or not final_dist_name:
        return {"error": "í•„ìˆ˜ ì¸ì('group_name', 'distribution_name')ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. ì‚¬ìš©ìì—ê²Œ ì–´ë–¤ ê·¸ë£¹ì„ ë¶„ì„í• ì§€ ë¬¼ì–´ë³´ì„¸ìš”."}

    try:
        b_lives_list = parse_blives_input(args.get('b_lives', b_lives))
        failure_prob_times_list = [float(x) for x in parse_list_input(args.get('failure_prob_times', failure_prob_times))]
    except Exception as e:
        return {"error": f"B-ìˆ˜ëª… ë˜ëŠ” ê³ ì¥ í™•ë¥  ì‹œê°„ ë¦¬ìŠ¤íŠ¸ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}"}

    if 'grouped_data' not in st.session_state:
        return {"error": "ë°ì´í„°ê°€ ë¨¼ì € ìš”ì•½ë˜ì–´ì•¼ í•©ë‹ˆë‹¤."}
        
    try:
        output_buffer = io.StringIO()
        with contextlib.redirect_stdout(output_buffer):
            # 1. analyze_single_distributionìœ¼ë¡œë¶€í„° fitter ê°ì²´ë¥¼ ì§ì ‘ ë°›ìŒ
            fitter_obj = analyze_single_distribution(
                st.session_state['grouped_data'], str(final_group_name), final_dist_name, 
                b_lives_list, failure_prob_times_list
            )
        
        # 2. fitter ê°ì²´ì˜ .results ì†ì„±ì„ ì‚¬ìš©í•˜ì—¬ DataFrameì„ ìƒì„±
        # ì´ ë¶€ë¶„ì€ RA_code_v6.pyì˜ analyze_single_distributionì´ fitterë¥¼ ë°˜í™˜í•œ í›„
        # printí•˜ë˜ ë¡œì§ì„ toolë¡œ ì˜®ê²¨ì˜¨ ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.
        results = {}
        params, values = [], []
        if final_dist_name == "Weibull_2P":
            params.extend(['Alpha', 'Beta'])
            values.extend([fitter_obj.alpha, fitter_obj.beta])
        elif final_dist_name in ["Lognormal_2P", "Normal_2P"]:
            params.extend(['Mu', 'Sigma'])
            values.extend([fitter_obj.mu, fitter_obj.sigma])
        elif final_dist_name == "Exponential_1P":
            params.append('Lambda')
            values.append(fitter_obj.Lambda)
        results['Parameter'] = params
        results['Value'] = values

        if b_lives_list:
            b_life_values = [fitter_obj.distribution.quantile(b/100) for b in b_lives_list]
            results['B-Life'] = [f'B{b}' for b in b_lives_list]
            results['B-Life Value'] = b_life_values
            
        if failure_prob_times_list:
            probs = [fitter_obj.distribution.CDF(t, show_plot=False) for t in failure_prob_times_list]
            results['Time'] = failure_prob_times_list
            results['Failure Probability'] = probs

        results_df = pd.DataFrame({k: pd.Series(v) for k, v in results.items()})

        plot_path = os.path.join("results", f"probability_plot_{final_group_name}_{final_dist_name}.png")
        return {"dataframe": results_df.to_dict('records'), "results_text": output_buffer.getvalue(), "plot_path": plot_path}
    except Exception as e:
        return {"error": f"ìƒì„¸ ë¶„í¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}

@tool
def parameter_homogeneity_checker_tool(distribution_name: Union[str, dict]) -> Dict[str, Any]:
    """ì‚¬ìš©ìê°€ íŠ¹ì • ë¶„í¬(Weibull, Lognormal, Normal)ë¥¼ ì§€ì •í•˜ì—¬ ê·¸ë£¹ ê°„ í˜•ìƒëª¨ìˆ˜ì˜ ë™ì¼ì„± ê²€í† ë¥¼ ìš”ì²­í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    ë‚´ë¶€ì ìœ¼ë¡œ ê° ê·¸ë£¹ì— ëŒ€í•´ í•´ë‹¹ ë¶„í¬ë¡œ ìƒì„¸ ë¶„ì„ì„ ìˆ˜í–‰í•œ í›„, ê·¸ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ìˆ˜ ë™ì¼ì„±ì„ ê²€í† í•©ë‹ˆë‹¤."""
    st.write(f"ğŸ§° **Tool Executing:** `parameter_homogeneity_checker_tool`")

    try:
        dist_name_str = parse_string_input(distribution_name).lower()
    except Exception as e:
        return {"error": f"ë¶„í¬ ì´ë¦„ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}"}

    # 1. ë¶„í¬ ì´ë¦„ì— ë”°ë¼ ê²€ì •í•  íŒŒë¼ë¯¸í„°ì™€ fitter ì´ë¦„ì„ ê²°ì •
    if 'weibull' in dist_name_str:
        parameter_to_check = 'beta'
        selected_dist = 'Weibull_2P'
    elif 'lognormal' in dist_name_str:
        parameter_to_check = 'sigma'
        selected_dist = 'Lognormal_2P'
    elif 'normal' in dist_name_str:
        parameter_to_check = 'sigma'
        selected_dist = 'Normal_2P'
    elif 'exponential' in dist_name_str:
        return {"error": "ì§€ìˆ˜ë¶„í¬ëŠ” ë‹¨ì¼ ëª¨ìˆ˜ ë¶„í¬ì´ë¯€ë¡œ í˜•ìƒ ëª¨ìˆ˜ ë™ì¼ì„± ê²€ì •ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    else:
        return {"error": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¶„í¬ì…ë‹ˆë‹¤: '{dist_name_str}'. Weibull, Lognormal, Normal ì¤‘ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”."}

    if 'grouped_data' not in st.session_state or not st.session_state['grouped_data']:
        return {"error": "ì„¸ì…˜ì— grouped_dataê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ìš”ì•½ì„ ë¨¼ì € ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤."}

    grouped_data = st.session_state['grouped_data']
    
    # 2. ê° ê·¸ë£¹ì— ëŒ€í•´ ê°œë³„ ìƒì„¸ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì—¬ detailed_analysis_results ê°ì²´ ìƒì„±
    st.write(f"â³ ê° ê·¸ë£¹ì— ëŒ€í•´ '{selected_dist}' ë¶„í¬ë¡œ ìƒì„¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
    detailed_analysis_results = {}
    try:
        for group_name in grouped_data.keys():
            with io.StringIO() as buf, contextlib.redirect_stdout(buf):
                fitter_result = analyze_single_distribution(
                    grouped_data=grouped_data,
                    group_name=group_name,
                    distribution_name=selected_dist,
                    b_lives=[],
                    failure_prob_times=[]
                )
            # check_parameter_homogeneityê°€ ê¸°ëŒ€í•˜ëŠ” ì´ì¤‘ ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°ë¡œ ì €ì¥
            detailed_analysis_results[group_name] = {selected_dist: fitter_result}
            #detailed_analysis_results[group_name] = fitter_result

        st.session_state['detailed_analysis_results'] = detailed_analysis_results
        st.write("âœ… ëª¨ë“  ê·¸ë£¹ì˜ ìƒì„¸ ë¶„ì„ ì™„ë£Œ.")

    except Exception as e:
        return {"error": f"ìƒì„¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}

    # ###################
    # st.write(f"(1ì°¨)ğŸ”¬ ìµœì¢… '{parameter_to_check}' ëª¨ìˆ˜ ë™ì¼ì„± ê²€ì •ì„ ì‹¤í–‰í•©ë‹ˆë‹¤...")
    # try:
    #     param_values = []
    #     for group, dists in detailed_analysis_results.items():
    #         first_dist_name = next(iter(dists))
    #         fitter = dists[first_dist_name]
            
    #         st.write(fitter)
    #         st.write(fitter.distribution)
    #         st.write(fitter.beta)
    #         st.write(fitter.beta_lower)
    #         st.write(fitter.beta_upper)
            
    #         # --- More robustly check for parameter and its confidence interval ---
    #         if hasattr(fitter, parameter_to_check):
    #             val = getattr(fitter, parameter_to_check)
    #             lower = getattr(fitter, f"{parameter_to_check}_lower", None)
    #             upper = getattr(fitter, f"{parameter_to_check}_upper", None)
                
    #             st.write(val)
    #             st.write(lower)
    #             st.write(upper)
                
    #             param_values.append({'Group': group, 'Parameter': parameter_to_check, 'Value': val, 'Lower CI': lower, 'Upper CI': upper})
                
    #             st.write(param_values)
        
    #     if not param_values: raise ValueError(f"ëª¨ìˆ˜ '{parameter_to_check}' ë˜ëŠ” í•´ë‹¹ ì‹ ë¢°êµ¬ê°„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    #     ###################
    # except Exception as e:
    #     return {"error": f"(1ì°¨) ëª¨ìˆ˜ ë™ì¼ì„± ê²€í†  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}

    # 3. ìƒì„±ëœ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ìˆ˜ ë™ì¼ì„± ê²€ì • ì‹¤í–‰
    st.write(f"ğŸ”¬ ìµœì¢… '{parameter_to_check}' ëª¨ìˆ˜ ë™ì¼ì„± ê²€ì •ì„ ì‹¤í–‰í•©ë‹ˆë‹¤...")
    try:
        output_buffer = io.StringIO()
        with contextlib.redirect_stdout(output_buffer):
            results_df = check_parameter_homogeneity(
                analysis_results=detailed_analysis_results,
                parameter_to_check=parameter_to_check
            )
        
        plot_path = os.path.join("results", f"contour_plot_{parameter_to_check}.png")
        return {
            "dataframe": results_df.to_dict('records'), 
            "results_text": output_buffer.getvalue(), 
            "plot_path": plot_path
        }
    except Exception as e:
        return {"error": f"ëª¨ìˆ˜ ë™ì¼ì„± ê²€í†  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}
